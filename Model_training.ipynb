{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "from typing import Tuple, List, NoReturn, Union, Any\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parsed(dump_features_location: str = \"../train_data\",\n",
    "                dump_features_fn: str = \"train_dataset.npz\"):\n",
    "    data = np.load(os.path.join(dump_features_location, dump_features_fn))\n",
    "    features, outputs = data['features'], data['outputs']\n",
    "    print(f\"Features shape: {features.shape}\")\n",
    "    print(f\"Outputs shape: {outputs.shape}\")\n",
    "\n",
    "    return features, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (274818, 28, 11)\n",
      "Outputs shape: (274818, 28)\n"
     ]
    }
   ],
   "source": [
    "data_input, data_output = load_parsed(\"train_data\", \"train_dataset.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Net(object):\n",
    "    FIRST_LAYER_FILTERS = 16\n",
    "    FIRST_LAYER_KERNEL = 2\n",
    "    SECOND_LAYER_FILTERS = 32\n",
    "    SECOND_LAYER_KERNEL = 2\n",
    "\n",
    "    def __init__(self, input_size: Tuple[int, int], output_size: int,\n",
    "                 cached_model: bool, cached_model_path: str, seed=None):\n",
    "        np.random.seed(seed)\n",
    "        if cached_model:\n",
    "            # returns an already compiled model\n",
    "            self._model = load_model(cached_model_path)\n",
    "        else:\n",
    "            # returns an already compiled model\n",
    "            self._model = self.create_model(input_size, output_size,\n",
    "                                            loss=SparseCategoricalCrossentropy(),\n",
    "                                            optimizer=\"adam\",\n",
    "                                            metrics=['accuracy'], verbose=True)\n",
    "\n",
    "\n",
    "    def create_model(self, input_size: Tuple[int, int], output_size: int,\n",
    "                     loss: Union[str, Any], optimizer: str, metrics: List[str],\n",
    "                    verbose: bool):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(self.FIRST_LAYER_FILTERS, self.FIRST_LAYER_KERNEL,\n",
    "                               activation='relu', batch_input_shape=(None, *input_size))) #input_shape=input_size\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D())\n",
    "        model.add(Conv1D(self.SECOND_LAYER_FILTERS, self.SECOND_LAYER_KERNEL,\n",
    "                               activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(output_size, activation='sigmoid'))\n",
    "        if verbose:\n",
    "            print(\"Model created:\")\n",
    "            print(model.summary())\n",
    "\n",
    "        self._loss = loss\n",
    "        self._metrics = metrics\n",
    "        model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "        return model\n",
    "\n",
    "    def train(self, input_train, target_train, validation_split: float,\n",
    "              n_epochs: int, batch_size: int, verbose: int,\n",
    "              model_version: str)-> NoReturn:\n",
    "\n",
    "        # Fit data to model\n",
    "        history = self._model.fit(input_train, target_train,\n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs=n_epochs,\n",
    "                                  verbose=verbose,\n",
    "                                  validation_split=validation_split,\n",
    "                                  shuffle=True)\n",
    "\n",
    "        # Plot history\n",
    "        print(f\"History consist of: {history.keys()}\")\n",
    "        current_directory = os.path.dirname(os.path.abspath(__file__))\n",
    "        hist_df = pd.DataFrame(history, columns=self._metrics + ['loss', 'val_loss'])\n",
    "        fig = hist_df.plot(x=hist_df.index, y=['loss', 'val_loss']).get_figure()\n",
    "        curve_path = os.path.join(current_directory, os.path.pardir, \"models\", \"training_plot.png\")\n",
    "        fig.savefig(curve_path)\n",
    "\n",
    "        # Save model\n",
    "        self.save(model_save_fn=\"cnn_model_\" + model_version + \".hd5\")\n",
    "\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        predictions = self._model.predict(input_data)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def save(self, model_save_path=\"../models\",\n",
    "             model_save_fn=\"cnn_model.hd5\"):\n",
    "        self._model.save(os.path.join(model_save_path, model_save_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created:\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 27, 16)            368       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 27, 16)            64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 13, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 12, 32)            1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 32)            128       \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 28)                924       \n",
      "=================================================================\n",
      "Total params: 2,540\n",
      "Trainable params: 2,444\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.CNN_Net at 0x18efe50d708>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_Net(input_size=data_input.shape[1:], output_size=28, cached_model=False, cached_model_path=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 11)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
